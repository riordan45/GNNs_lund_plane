{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "worse-sport",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os.path as osp\n",
    "import os\n",
    "import glob\n",
    "import torch\n",
    "import awkward as ak\n",
    "import time\n",
    "import uproot\n",
    "import uproot3\n",
    "import numpy as np\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "from torch_geometric.datasets import MNISTSuperpixels\n",
    "from torch_geometric.data import DataListLoader, DataLoader\n",
    "import torch_geometric.transforms as T\n",
    "from torch_geometric.nn import SplineConv, global_mean_pool, DataParallel, EdgeConv\n",
    "from torch_geometric.data import Data\n",
    "from torchsummary import summary\n",
    "#from sklearn.neighbors import NearestNeighbors\n",
    "from tensorflow.keras.utils import to_categorical, plot_model\n",
    "#from sklearn.neighbors import kneighbors_graph\n",
    "import scipy.sparse as ss\n",
    "from datetime import datetime, timedelta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "liquid-prescription",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_train_dataset_prmy(z, k, d, label):\n",
    "    graphs = []\n",
    "    for i in range(len(z)):\n",
    "        vec = []\n",
    "        vec.append(np.array([d[i], z[i], k[i]]).T)\n",
    "        vec = np.array(vec)\n",
    "        vec = np.squeeze(vec)\n",
    "        ya = kneighbors_graph(vec, n_neighbors=int(np.floor(vec.shape[0]/2)))\n",
    "        edges = np.array([ya.nonzero()[0], ya.nonzero()[1]])\n",
    "        edge = torch.tensor(edges, dtype=torch.long)\n",
    "        graphs.append(Data(x=torch.tensor(vec, dtype=torch.float), edge_index=edge, y=torch.tensor(label[i], dtype=torch.float)))\n",
    "    return graphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "applicable-alcohol",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_train_dataset_fulld(z, k, d, p1, p2, label):\n",
    "    graphs = []\n",
    "    for i in range(len(z)):\n",
    "        if i%1000 == 0: \n",
    "            print(\"Processing event {}/{}\".format(i, len(z)), end=\"\\r\")\n",
    "        vec = []\n",
    "        vec.append(np.array([d[i], z[i], k[i]]).T)\n",
    "        vec = np.array(vec)\n",
    "        vec = np.squeeze(vec)\n",
    "\n",
    "        v1 = [[ind, x] for ind, x in enumerate(p1[i]) if x > -1]\n",
    "        v2 = [[ind, x] for ind, x in enumerate(p2[i]) if x > -1]\n",
    "\n",
    "        a1 = np.reshape(v1,(len(v1),2)).T\n",
    "        a2 = np.reshape(v2,(len(v2),2)).T\n",
    "        edge1 = np.concatenate((a1[0], a2[0], a1[1], a2[1]),axis = 0)\n",
    "        edge2 = np.concatenate((a1[1], a2[1], a1[0], a2[0]),axis = 0)\n",
    "        edge = torch.tensor(np.array([edge1, edge2]), dtype=torch.long)\n",
    "        graphs.append(Data(x=torch.tensor(vec, dtype=torch.float), edge_index=edge, y=torch.tensor(label[i], dtype=torch.float)))\n",
    "    return graphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "obvious-approval",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_test_dataset_prmy(z, k, d):\n",
    "    graphs = []\n",
    "    for i in range(len(z)):\n",
    "        vec = []\n",
    "        vec.append(np.array([d[i], z[i], k[i]]).T)\n",
    "        vec = np.array(vec)\n",
    "        vec = np.squeeze(vec)\n",
    "        ya = kneighbors_graph(vec, n_neighbors=int(np.floor(vec.shape[0]/2)))\n",
    "        edges = np.array([ya.nonzero()[0], ya.nonzero()[1]])\n",
    "        edge = torch.tensor(edges, dtype=torch.long)\n",
    "        graphs.append(Data(x=torch.tensor(vec, dtype=torch.float), edge_index=edge))\n",
    "    return graphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "undefined-finding",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_test_dataset_fulld(z, k, d, p1, p2):\n",
    "    graphs = []\n",
    "    for i in range(len(z)):\n",
    "        vec = []\n",
    "        vec.append(np.array([d[i], z[i], k[i]]).T)\n",
    "        vec = np.array(vec)\n",
    "        vec = np.squeeze(vec)\n",
    "        v1 = [[ind, x] for ind, x in enumerate(p1[i]) if x > -1]\n",
    "        v2 = [[ind, x] for ind, x in enumerate(p2[i]) if x > -1]\n",
    "\n",
    "        a1 = np.reshape(v1,(len(v1),2)).T\n",
    "        a2 = np.reshape(v2,(len(v2),2)).T\n",
    "        edge1 = np.concatenate((a1[0], a2[0], a1[1], a2[1]),axis = 0)\n",
    "        edge2 = np.concatenate((a1[1], a2[1], a1[0], a2[0]),axis = 0)\n",
    "        edge = torch.tensor(np.array([edge1, edge2]), dtype=torch.long)\n",
    "        graphs.append(Data(x=torch.tensor(vec, dtype=torch.float), edge_index=edge))\n",
    "    return graphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "appropriate-writer",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.conv1 = EdgeConv(nn.Sequential(nn.Linear(6, 64),\n",
    "                                  nn.ReLU(), nn.Linear(64, 64), nn.ReLU(), nn.Linear(64, 64)),aggr='add')\n",
    "        self.conv2 = EdgeConv(nn.Sequential(nn.Linear(128, 128),\n",
    "                                  nn.ReLU(), nn.Linear(128, 128),nn.ReLU(), nn.Linear(128, 128)),aggr='add')\n",
    "        self.conv3 = EdgeConv(nn.Sequential(nn.Linear(256,256,),\n",
    "                                  nn.ReLU(), nn.Linear(256, 256),nn.ReLU(), nn.Linear(256, 256)),aggr='add')\n",
    "        self.lin1 = torch.nn.Linear(256, 128)\n",
    "        self.lin2 = torch.nn.Linear(256, 64)\n",
    "        self.lin3 = torch.nn.Linear(64, 2)\n",
    "        \n",
    "    def forward(self, data):\n",
    "        x, edge_index, batch = data.x, data.edge_index, data.batch\n",
    "        x = self.conv1(x, edge_index)\n",
    "        x = self.conv2(x, edge_index)\n",
    "        x = self.conv3(x, edge_index)\n",
    "        x = global_mean_pool(x, batch)\n",
    "        x = F.dropout(x, p=0.1)\n",
    "        x = self.lin2(x)\n",
    "        x = F.dropout(x, p=0.1)\n",
    "        x = self.lin3(x)\n",
    "        #print(x.shape)\n",
    "        return F.sigmoid(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "technological-cycle",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Adversary(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Adversary, self).__init__()\n",
    "        self.lin1 = torch.nn.Linear(2, 32)\n",
    "        self.lin2 = torch.nn.Linear(32, 64)\n",
    "        self.lin3 = torch.nn.Linear(64, 128)\n",
    "        self.lin4 = torch.nn.Linear(128, 256)\n",
    "        \n",
    "    def forward(self, data):\n",
    "        x = data\n",
    "        x = self.lin1(x)\n",
    "        x = self.lin2(x)\n",
    "        x = self.lin3(x)\n",
    "        x = self.lin4(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "speaking-slovakia",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 0.0002\n",
    "beta_1 = 0.5\n",
    "beta_2 = 0.999\n",
    "device = torch.device('cuda:2')\n",
    "gen = Net().to(device)\n",
    "gen_opt = torch.optim.Adam(gen.parameters(), lr=lr, betas=(beta_1, beta_2))\n",
    "adv = Adversary().to(device)\n",
    "adv_opt = torch.optim.Adam(adv.parameters(), lr=lr, betas=(beta_1, beta_2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "hazardous-decade",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_epochs = 50\n",
    "cur_step = 0\n",
    "mean_generator_loss = 0\n",
    "mean_discriminator_loss = 0\n",
    "# I still have to work on this,\n",
    "# it's just a GAN so not really a proper adversarial network\n",
    "# I have to implement a Gradient Reversal Layer like in the adversarial package from\n",
    "# the looks of it which will take a bit of time\n",
    "for epoch in range(n_epochs):\n",
    "    # Dataloader returns the batches\n",
    "    for real in dataloader:\n",
    "        cur_batch_size = len(real)\n",
    "        real = real.to(device)\n",
    "\n",
    "        ## Update discriminator ##\n",
    "        disc_opt.zero_grad()\n",
    "        fake_noise = get_noise(cur_batch_size, z_dim, device=device)\n",
    "        fake = gen(fake_noise)\n",
    "        disc_fake_pred = disc(fake.detach())\n",
    "        disc_fake_loss = criterion(disc_fake_pred, torch.zeros_like(disc_fake_pred))\n",
    "        disc_real_pred = disc(real)\n",
    "        disc_real_loss = criterion(disc_real_pred, torch.ones_like(disc_real_pred))\n",
    "        disc_loss = (disc_fake_loss + disc_real_loss) / 2\n",
    "\n",
    "        # Keep track of the average discriminator loss\n",
    "        mean_discriminator_loss += disc_loss.item() / display_step\n",
    "        # Update gradients\n",
    "        disc_loss.backward(retain_graph=True)\n",
    "        # Update optimizer\n",
    "        disc_opt.step()\n",
    "\n",
    "        ## Update generator ##\n",
    "        gen_opt.zero_grad()\n",
    "        fake_noise_2 = get_noise(cur_batch_size, z_dim, device=device)\n",
    "        fake_2 = gen(fake_noise_2)\n",
    "        disc_fake_pred = disc(fake_2)\n",
    "        gen_loss = criterion(disc_fake_pred, torch.ones_like(disc_fake_pred))\n",
    "        gen_loss.backward()\n",
    "        gen_opt.step()\n",
    "\n",
    "        # Keep track of the average generator loss\n",
    "        mean_generator_loss += gen_loss.item() / display_step\n",
    "\n",
    "        ## Visualization code ##\n",
    "        if cur_step % display_step == 0 and cur_step > 0:\n",
    "            print(f\"Step {cur_step}: Generator loss: {mean_generator_loss}, discriminator loss: {mean_discriminator_loss}\")\n",
    "            mean_generator_loss = 0\n",
    "            mean_discriminator_loss = 0\n",
    "        cur_step += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "collaborative-conditioning",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Configuration from bash script\n",
    "if \"INFILE\" in os.environ:\n",
    "    infile_path = os.environ[\"INFILE\"]\n",
    "    model_name = os.environ[\"MODEL\"]\n",
    "    epochs = int(os.environ[\"EPOCHS\"])\n",
    "\n",
    "#Configuration in notebook\n",
    "else:    \n",
    "#    files = glob.glob(\"/mnt/storage/asopio/train_test_split_20210305/training_set_jz2to9.root\")\n",
    "    files = glob.glob(\"/mnt/storage/raresiora/train_test_split/full_decluster/zprime/*_train.root\") + glob.glob(\"/mnt/storage/raresiora/train_test_split/full_decluster/j3to9/*_train.root\")\n",
    "    model_name = \"GNN_full\"\n",
    "#    model_name = \"LSTM\"\n",
    "#    model_name = \"1DCNN\"\n",
    "#    model_name = \"2DCNN\"\n",
    "#    model_name = \"ImgCNN\"\n",
    "#    model_name = \"GNN\"\n",
    "#    model_name = \"Transformer\"\n",
    "#    nb_epochs = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "realistic-trinidad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/mnt/storage/raresiora/train_test_split/full_decluster/zprime/user.asopio.24603668._000012.ANALYSIS.root_train.root\n",
      "Evaluating on 16 files with 1066593 entries in total.\n"
     ]
    }
   ],
   "source": [
    "file1 = glob.glob(\"/mnt/storage/raresiora/train_test_split/primary_plane/wprime/*_train.root\")\n",
    "file2 = glob.glob(\"/mnt/storage/raresiora/train_test_split/primary_plane/zprime/*_train.root\")\n",
    "file3 = glob.glob(\"/mnt/storage/raresiora/train_test_split/primary_plane/j3to9/*_train.root\")\n",
    "nentries_total = 0\n",
    "intreename = \"lundjets_InDetTrackParticles\"\n",
    "print(files[0])\n",
    "for infile_name in files: \n",
    "    nentries_total += uproot3.numentries(infile_name, intreename)\n",
    "\n",
    "print(\"Evaluating on {} files with {} entries in total.\".format(len(file3), nentries_total))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "existing-aggregate",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load tf keras model\n",
    "# jet_type = \"Akt10RecoChargedJet\" #track jets\n",
    "jet_type = \"Akt10UFOJet\" #UFO jets\n",
    "\n",
    "save_trained_model = True\n",
    "intreename = \"lundjets_InDetTrackParticles\"\n",
    "\n",
    "model_filename = \"save/models/\"+model_name+\".hdf5\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "meaningful-watts",
   "metadata": {},
   "source": [
    "Full declustering data loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "tutorial-williams",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training tagger on files 20\n",
      "Loading file /mnt/storage/raresiora/train_test_split/full_decluster/zprime/user.asopio.24603668._000012.ANALYSIS.root_train.root\n",
      "Loading file /mnt/storage/raresiora/train_test_split/full_decluster/zprime/user.asopio.24603668._000005.ANALYSIS.root_train.root\n",
      "Loading file /mnt/storage/raresiora/train_test_split/full_decluster/zprime/user.asopio.24603668._000019.ANALYSIS.root_train.root\n",
      "Loading file /mnt/storage/raresiora/train_test_split/full_decluster/j3to9/user.asopio.24603633._000004.ANALYSIS.root_train.root\n",
      "Loading file /mnt/storage/raresiora/train_test_split/full_decluster/j3to9/user.asopio.24603631._000012.ANALYSIS.root_train.root\n",
      "Loading file /mnt/storage/raresiora/train_test_split/full_decluster/j3to9/user.asopio.24603627._000002.ANALYSIS.root_train.root\n",
      "Loading file /mnt/storage/raresiora/train_test_split/full_decluster/j3to9/user.asopio.24603627._000008.ANALYSIS.root_train.root\n",
      "Loading file /mnt/storage/raresiora/train_test_split/full_decluster/j3to9/user.asopio.24603626._000018.ANALYSIS.root_train.root\n",
      "Loading file /mnt/storage/raresiora/train_test_split/full_decluster/j3to9/user.asopio.24603630._000002.ANALYSIS.root_train.root\n",
      "Loading file /mnt/storage/raresiora/train_test_split/full_decluster/j3to9/user.asopio.24603628._000032.ANALYSIS.root_train.root\n",
      "Loading file /mnt/storage/raresiora/train_test_split/full_decluster/j3to9/user.asopio.24603626._000004.ANALYSIS.root_train.root\n",
      "Loading file /mnt/storage/raresiora/train_test_split/full_decluster/j3to9/user.asopio.24603632._000003.ANALYSIS.root_train.root\n",
      "Loading file /mnt/storage/raresiora/train_test_split/full_decluster/j3to9/user.asopio.24603628._000031.ANALYSIS.root_train.root\n",
      "Loading file /mnt/storage/raresiora/train_test_split/full_decluster/j3to9/user.asopio.24603630._000009.ANALYSIS.root_train.root\n",
      "Loading file /mnt/storage/raresiora/train_test_split/full_decluster/j3to9/user.asopio.24603632._000007.ANALYSIS.root_train.root\n",
      "Loading file /mnt/storage/raresiora/train_test_split/full_decluster/j3to9/user.asopio.24603628._000046.ANALYSIS.root_train.root\n",
      "Loading file /mnt/storage/raresiora/train_test_split/full_decluster/j3to9/user.asopio.24603632._000002.ANALYSIS.root_train.root\n",
      "Loading file /mnt/storage/raresiora/train_test_split/full_decluster/j3to9/user.asopio.24603631._000003.ANALYSIS.root_train.root\n",
      "Loading file /mnt/storage/raresiora/train_test_split/full_decluster/j3to9/user.asopio.24603627._000011.ANALYSIS.root_train.root\n",
      "Loading file /mnt/storage/raresiora/train_test_split/full_decluster/j3to9/user.asopio.24603631._000007.ANALYSIS.root_train.root\n",
      "Opened data in 3258.9297 seconds.\n"
     ]
    }
   ],
   "source": [
    "def pad_ak(arr, l):\n",
    "    arr = ak.pad_none(arr, l)\n",
    "    arr = ak.fill_none(arr, 0)\n",
    "    arr = ak.to_numpy(arr)\n",
    "    return arr\n",
    "\n",
    "def pad_ak3(arr, l):\n",
    "    arr = ak.pad_none(arr, 1)\n",
    "    arr = ak.fill_none(arr, [0])\n",
    "    arr = arr[:,0]\n",
    "    arr = ak.pad_none(arr, l)\n",
    "    arr = ak.fill_none(arr, 0)\n",
    "    #arr = ak.to_numpy(arr)\n",
    "    return arr\n",
    "\n",
    "print(\"Training tagger on files\", len(files))\n",
    "t_start = time.time()\n",
    "\n",
    "dsids = np.array([])\n",
    "NBHadrons = np.array([])\n",
    "trial = np.ones((1,30))\n",
    "all_lund_zs = ak.from_numpy(trial)\n",
    "all_lund_kts = ak.from_numpy(trial)\n",
    "all_lund_drs = ak.from_numpy(trial)\n",
    "parent1 = ak.from_numpy(trial)\n",
    "parent2 = ak.from_numpy(trial)\n",
    "vector = []\n",
    "for file in files:\n",
    "    \n",
    "    print(\"Loading file\",file)\n",
    "    \n",
    "    infile = uproot.open(file)\n",
    "    tree = infile[intreename]\n",
    "    dsids = np.append( dsids, np.array(tree[\"DSID\"].array()) )\n",
    "    mcweights = tree[\"mcWeight\"].array()\n",
    "    NBHadrons = np.append( NBHadrons, pad_ak(tree[\"Akt10TruthJet_inputJetGABHadrons\"].array(), 30)[:,0] )\n",
    "    parent1 = ak.concatenate((parent1, pad_ak3(tree[\"{}_jetLundIDParent1\".format(jet_type)].array(), 2)), axis = 0)\n",
    "    parent2 = ak.concatenate((parent2, pad_ak3(tree[\"{}_jetLundIDParent2\".format(jet_type)].array(), 2)), axis = 0)\n",
    "    #Get jet kinematics\n",
    "    \n",
    "    all_lund_zs = ak.concatenate((all_lund_zs, pad_ak3(tree[\"{}_jetLundZ\".format(jet_type)].array(), 2)), axis=0)\n",
    "    all_lund_kts = ak.concatenate((all_lund_kts, pad_ak3(tree[\"{}_jetLundKt\".format(jet_type)].array(), 2)), axis=0)\n",
    "    all_lund_drs = ak.concatenate((all_lund_drs, pad_ak3(tree[\"{}_jetLundDeltaR\".format(jet_type)].array(), 2)), axis=0)\n",
    "    \n",
    "all_lund_zs = all_lund_zs[1:]    \n",
    "all_lund_kts = all_lund_kts[1:]\n",
    "all_lund_drs = all_lund_drs[1:]\n",
    "parent1 = parent1[1:]\n",
    "parent2 = parent2[1:]\n",
    "\n",
    "delta_t_fileax = time.time() - t_start\n",
    "print(\"Opened data in {:.4f} seconds.\".format(delta_t_fileax))\n",
    "\n",
    "#Get labels\n",
    "#labels = ( dsids > 360000 ) & ( dsids < 370000 )\n",
    "\n",
    "labels = ( dsids > 370000 ) & ( NBHadrons > 0 ) # do NBHadrons == 0 for W bosons, NBHadrons > 0 for Tops\n",
    "\n",
    "\n",
    "#print(labels)\n",
    "labels = to_categorical(labels, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "psychological-settle",
   "metadata": {},
   "source": [
    "Primary plane data loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "funky-banner",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This part is self contained. I should rename the functions though.\n",
    "# It works f\n",
    "def pad_ak(arr, l):\n",
    "    arr = ak.pad_none(arr, l)\n",
    "    arr = ak.fill_none(arr, 0)\n",
    "    arr = ak.to_numpy(arr)\n",
    "    return arr\n",
    "\n",
    "def pad_ak3(arr, l):\n",
    "    arr = ak.pad_none(arr, 1)\n",
    "    arr = ak.fill_none(arr, [0])\n",
    "    arr = arr[:,0]\n",
    "    arr = ak.pad_none(arr, l)\n",
    "    arr = ak.fill_none(arr, 0)\n",
    "    #arr = ak.to_numpy(arr)\n",
    "    return arr\n",
    "\n",
    "print(\"Training tagger on files\", len(files))\n",
    "t_start = time.time()\n",
    "dsids = np.array([])\n",
    "NBHadrons = np.array([])\n",
    "trial = np.ones((1,30))\n",
    "all_lund_zs = ak.from_numpy(trial)\n",
    "all_lund_kts = ak.from_numpy(trial)\n",
    "all_lund_drs = ak.from_numpy(trial)\n",
    "\n",
    "\n",
    "for file in files:\n",
    "    \n",
    "    print(\"Loading file\",file)\n",
    "    infile = uproot.open(file)\n",
    "    tree = infile[intreename]\n",
    "    dsids = np.append( dsids, np.array(tree[\"DSID\"].array()) )\n",
    "    mcweights = tree[\"mcWeight\"].array()\n",
    "    NBHadrons = np.append( NBHadrons, pad_ak(tree[\"Akt10TruthJet_inputJetGABHadrons\"].array(), 30)[:,0] )\n",
    "    #Get jet kinematics\n",
    "    \n",
    "    #The following 4 variables are irrelevant for training, might as well not load them here if you don't want\n",
    "    jet_pts = pad_ak(tree[\"AntiKt10UFOCSSKSoftDropBeta100Zcut10JetsCalib_jetPt\"].array(), 30)[:,0]\n",
    "    jet_etas = pad_ak(tree[\"AntiKt10UFOCSSKSoftDropBeta100Zcut10JetsCalib_jetEta\"].array(), 30)[:,0]\n",
    "    jet_phis = pad_ak(tree[\"AntiKt10UFOCSSKSoftDropBeta100Zcut10JetsCalib_jetPhi\"].array(), 30)[:,0]              \n",
    "    jet_ms = pad_ak(tree[\"AntiKt10UFOCSSKSoftDropBeta100Zcut10JetsCalib_jetM\"].array(), 30)[:,0]\n",
    "\n",
    "    all_lund_zs = ak.concatenate((all_lund_zs, pad_ak3(tree[\"{}_jetLundZ\".format(jet_type)].array(), 2)), axis=0)\n",
    "    all_lund_kts = ak.concatenate((all_lund_kts, pad_ak3(tree[\"{}_jetLundKt\".format(jet_type)].array(), 2)), axis=0)\n",
    "    all_lund_drs = ak.concatenate((all_lund_drs, pad_ak3(tree[\"{}_jetLundDeltaR\".format(jet_type)].array(), 2)), axis=0)\n",
    "    \n",
    "all_lund_zs = all_lund_zs[1:]    \n",
    "all_lund_kts = all_lund_kts[1:]\n",
    "all_lund_drs = all_lund_drs[1:]\n",
    "    \n",
    "delta_t_fileax = time.time() - t_start\n",
    "print(\"Opened data in {:.4f} seconds.\".format(delta_t_fileax))\n",
    "\n",
    "#Get labels\n",
    "#labels = ( dsids > 360000 ) & ( dsids < 370000 )\n",
    "\n",
    "labels = ( dsids > 370000 ) # depends on your signal and background definition\n",
    "\n",
    "\n",
    "# print(labels)\n",
    "labels = to_categorical(labels, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "bottom-binding",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "305154\n"
     ]
    }
   ],
   "source": [
    "# This finds the number of trainable parameters for your model\n",
    "model = Net()\n",
    "total = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(total)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "prostate-motion",
   "metadata": {},
   "source": [
    "For Primary plane data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "impaired-immigration",
   "metadata": {},
   "outputs": [],
   "source": [
    "#W bosons\n",
    "dataset = create_train_dataset_prmy(all_lund_zs, all_lund_kts, all_lund_drs, labels) \n",
    "train_loader = DataLoader(dataset, batch_size=1024, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "selected-delight",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Top Quark\n",
    "dataset2 = create_train_dataset_prmy(all_lund_zs, all_lund_kts, all_lund_drs, labels)\n",
    "train_loader = DataLoader(dataset2, batch_size=1024, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "occupied-surprise",
   "metadata": {},
   "source": [
    "For Full declustering data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "attractive-occurrence",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing event 1242000/1242295\r"
     ]
    }
   ],
   "source": [
    "#W bosons\n",
    "# It will take about 30 minutes to finish\n",
    "dataset = create_train_dataset_fulld(all_lund_zs, all_lund_kts, all_lund_drs, parent1, parent2, labels) \n",
    "train_loader = DataLoader(dataset, batch_size=1024, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "radical-literacy",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing event 1066000/1066593\r"
     ]
    }
   ],
   "source": [
    "#Top Quark\n",
    "# It will take about 25 minutes to finish\n",
    "dataset = create_train_dataset_fulld(all_lund_zs, all_lund_kts, all_lund_drs, parent1, parent2, labels)\n",
    "train_loader = DataLoader(dataset, batch_size=1024, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "allied-winter",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Boom\n"
     ]
    }
   ],
   "source": [
    "print(\"Boom\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "imported-hypothetical",
   "metadata": {},
   "source": [
    "Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "celtic-updating",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:1\n",
      "Epoch: 001, Loss: 0.23922, Train Acc: 0.90895\n",
      "Epoch:2\n",
      "Epoch: 002, Loss: 0.21002, Train Acc: 0.90863\n",
      "Epoch:3\n",
      "Epoch: 003, Loss: 0.19958, Train Acc: 0.91428\n",
      "Epoch:4\n",
      "Epoch: 004, Loss: 0.19089, Train Acc: 0.92489\n",
      "Epoch:5\n",
      "Epoch: 005, Loss: 0.18531, Train Acc: 0.92552\n",
      "Epoch:6\n",
      "Epoch: 006, Loss: 0.18220, Train Acc: 0.91875\n",
      "Epoch:7\n",
      "Epoch: 007, Loss: 0.17972, Train Acc: 0.92899\n",
      "Epoch:8\n",
      "Epoch: 008, Loss: 0.17710, Train Acc: 0.92964\n",
      "Epoch:9\n",
      "Epoch: 009, Loss: 0.17598, Train Acc: 0.92867\n",
      "Epoch:10\n",
      "Epoch: 010, Loss: 0.17500, Train Acc: 0.92726\n",
      "Epoch:11\n",
      "Epoch: 011, Loss: 0.17329, Train Acc: 0.93061\n",
      "Epoch:12\n",
      "Epoch: 012, Loss: 0.17289, Train Acc: 0.92954\n",
      "Epoch:13\n",
      "Epoch: 013, Loss: 0.17114, Train Acc: 0.92458\n",
      "Epoch:14\n",
      "Epoch: 014, Loss: 0.17042, Train Acc: 0.93188\n",
      "Epoch:15\n",
      "Epoch: 015, Loss: 0.16937, Train Acc: 0.93072\n",
      "Epoch:16\n",
      "Epoch: 016, Loss: 0.16916, Train Acc: 0.93206\n",
      "Epoch:17\n",
      "Epoch: 017, Loss: 0.16708, Train Acc: 0.93219\n",
      "Epoch:18\n",
      "Epoch: 018, Loss: 0.16643, Train Acc: 0.93343\n",
      "Epoch:19\n",
      "Epoch: 019, Loss: 0.16580, Train Acc: 0.93336\n",
      "Epoch:20\n",
      "Epoch: 020, Loss: 0.16547, Train Acc: 0.93365\n"
     ]
    }
   ],
   "source": [
    "model = Net()\n",
    "device = torch.device('cuda:2') # Usually gpu 4 worked best, it had the most memory available\n",
    "model.to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.0005)\n",
    "\n",
    "def train(epoch):\n",
    "    model.train()\n",
    "\n",
    "    loss_all = 0\n",
    "    for data in train_loader:\n",
    "        data = data.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        new_y = torch.reshape(data.y, (int(list(data.y.shape)[0]/2),2))\n",
    "        loss = F.binary_cross_entropy(output, new_y)\n",
    "        loss.backward()\n",
    "        loss_all += data.num_graphs * loss.item()\n",
    "        optimizer.step()\n",
    "    return loss_all / len(dataset)\n",
    "\n",
    "@torch.no_grad()\n",
    "def get_accuracy(loader):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    for data in loader:\n",
    "        data = data.to(device)\n",
    "        new_y = torch.reshape(data.y, (int(list(data.y.shape)[0]/2),2))\n",
    "        pred = model(data).max(dim=1)[1]\n",
    "        correct += pred.eq(new_y[:,1]).sum().item()\n",
    "    return correct / len(loader.dataset)\n",
    "    \n",
    "@torch.no_grad()\n",
    "def get_scores(loader):\n",
    "    model.eval()\n",
    "    total_output = np.array([[1,1]])\n",
    "    for data in loader:\n",
    "        data = data.to(device)\n",
    "        pred = model(data)\n",
    "        total_output = np.append(total_output, pred.cpu().detach().numpy(), axis=0)\n",
    "    return total_output[1:]\n",
    "    \n",
    "for epoch in range(1, 21):\n",
    "    print(\"Epoch:{}\".format(epoch))\n",
    "    loss = train(epoch)\n",
    "    train_acc = get_accuracy(train_loader)\n",
    "    print('Epoch: {:03d}, Loss: {:.5f}, Train Acc: {:.5f}'.format(epoch, loss, train_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "regional-banking",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"/your/path/to/models/zprime.pt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "intellectual-banana",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "copyrighted-saver",
   "metadata": {},
   "source": [
    "# Evaluation part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "grand-definition",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating on 20 files with 9599237 entries in total.\n"
     ]
    }
   ],
   "source": [
    "#output ntuple directory\n",
    "# outdir = \"/mnt/storage/asopio\"\n",
    "outdir = \"your/path/to/scores\"\n",
    "model_name = \"GNN_fulld_20210523_t\"\n",
    "#input TTree name\n",
    "intreename = \"lundjets_InDetTrackParticles\"\n",
    "\n",
    "#infiles_list = glob.glob(infiles_bkg) + glob.glob(infiles_sig)\n",
    "#infiles_list = glob.glob(\"/mnt/storage/asopio/train_test_split_20210305/*ANALYSIS.root_test.root\")\n",
    "#file1 = glob.glob(\"/mnt/storage/asopio/train_test_split_20210305/wprime/*ANALYSIS.root_test.root\")\n",
    "#file2 = glob.glob(\"/mnt/storage/asopio/train_test_split_20210305/jz2/*ANALYSIS.root_test.root\")\n",
    "#file3 = glob.glob(\"/mnt/storage/asopio/train_test_split_20210305/jz3/*ANALYSIS.root_test.root\")\n",
    "#file4 = glob.glob(\"/mnt/storage/asopio/train_test_split_20210305/jz4/*ANALYSIS.root_test.root\")\n",
    "#file5 = glob.glob(\"/mnt/storage/asopio/train_test_split_20210305/jz5/*ANALYSIS.root_test.root\")\n",
    "#file6 = glob.glob(\"/mnt/storage/asopio/train_test_split_20210305/jz6/*ANALYSIS.root_test.root\")\n",
    "#file7 = glob.glob(\"/mnt/storage/asopio/train_test_split_20210305/jz7/*ANALYSIS.root_test.root\")\n",
    "#file8 = glob.glob(\"/mnt/storage/asopio/train_test_split_20210305/jz8/*ANALYSIS.root_test.root\")\n",
    "#file9 = glob.glob(\"/mnt/storage/asopio/train_test_split_20210305/jz9/*ANALYSIS.root_test.root\")\n",
    "#infiles_list = file1 + file3 + file4 + file5 + file6 + file7 + file8 + file9\n",
    "infiles_list = glob.glob(\"/mnt/storage/raresiora/train_test_split/full_decluster/zprime/*_test.root\") + glob.glob(\"/mnt/storage/raresiora/train_test_split/full_decluster/j3to9/*_test.root\")\n",
    "#files = glob.glob(\"/mnt/storage/raresiora/train_test_split/full_decluster/wprime/*_train.root\") + glob.glob(\"/mnt/storage/raresiora/train_test_split/full_decluster/j3to9/*_train.root\")\n",
    "nentries_total = 0\n",
    "for infile_name in infiles_list: \n",
    "    nentries_total += uproot3.numentries(infile_name, intreename)\n",
    "\n",
    "print(\"Evaluating on {} files with {} entries in total.\".format(len(infiles_list), nentries_total))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "baking-dependence",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GNN_fulld_20210523_t\n",
      "Loading file 1/20\n",
      "609645\n",
      "609645\n",
      "Opened data in 2621.6617 seconds.\n",
      "[[0.38414103 0.61684531]\n",
      " [0.99529809 0.00476372]\n",
      " [0.94582957 0.05464448]\n",
      " ...\n",
      " [0.38517058 0.61635953]\n",
      " [0.83747154 0.16330679]\n",
      " [0.97318876 0.02709857]]\n",
      "Calculated predicitions in 1214.9363 seconds,\n",
      "Saved data in 2.4441 seconds.\n",
      "Evaluated on 609645 out of 9599237 events\n",
      "Estimated time until completion: 15:43:29.218569\n",
      "Loading file 2/20\n",
      "631579\n",
      "631579\n",
      "Opened data in 2510.6316 seconds.\n",
      "[[0.335794   0.66515303]\n",
      " [0.90862072 0.09212638]\n",
      " [0.43054235 0.56968766]\n",
      " ...\n",
      " [0.00643208 0.99310815]\n",
      " [0.36816323 0.63195419]\n",
      " [0.02088805 0.97785711]]\n",
      "Calculated predicitions in 1257.9484 seconds,\n",
      "Saved data in 3840.0408 seconds.\n",
      "Evaluated on 1241224 out of 9599237 events\n",
      "Estimated time until completion: 14:13:54.143969\n",
      "Loading file 3/20\n",
      "186480\n",
      "186480\n",
      "Opened data in 745.8780 seconds.\n",
      "[[0.99894911 0.00103862]\n",
      " [0.49387866 0.50608742]\n",
      " [0.86327875 0.13911542]\n",
      " ...\n",
      " [0.1750643  0.82536817]\n",
      " [0.65403146 0.3457523 ]\n",
      " [0.77509767 0.22556061]]\n",
      "Calculated predicitions in 373.3655 seconds,\n",
      "Saved data in 7608.9277 seconds.\n",
      "Evaluated on 1427704 out of 9599237 events\n",
      "Estimated time until completion: 13:52:36.137787\n",
      "Loading file 4/20\n",
      "177549\n",
      "177549\n",
      "Opened data in 601.0905 seconds.\n",
      "[[9.99993086e-01 6.66565757e-06]\n",
      " [9.96600449e-01 3.41647537e-03]\n",
      " [9.99091625e-01 9.83783859e-04]\n",
      " ...\n",
      " [9.99999285e-01 8.02677732e-07]\n",
      " [9.99997377e-01 2.52418135e-06]\n",
      " [9.99349654e-01 6.36636396e-04]]\n",
      "Calculated predicitions in 340.7334 seconds,\n",
      "Saved data in 8728.4970 seconds.\n",
      "Evaluated on 1605253 out of 9599237 events\n",
      "Estimated time until completion: 13:22:37.160700\n",
      "Loading file 5/20\n",
      "974288\n",
      "974288\n",
      "Opened data in 3166.4125 seconds.\n",
      "[[8.44455838e-01 1.54319093e-01]\n",
      " [9.69142318e-01 3.06425262e-02]\n",
      " [7.94992805e-01 2.05670968e-01]\n",
      " ...\n",
      " [9.99671698e-01 3.44881468e-04]\n",
      " [9.91560161e-01 8.30188207e-03]\n",
      " [9.59827244e-01 4.08946127e-02]]\n",
      "Calculated predicitions in 1856.4903 seconds,\n",
      "Saved data in 9671.9374 seconds.\n",
      "Evaluated on 2579541 out of 9599237 events\n",
      "Estimated time until completion: 11:06:29.055903\n",
      "Loading file 6/20\n",
      "283106\n",
      "283106\n",
      "Opened data in 870.4680 seconds.\n",
      "[[0.99832731 0.00159194]\n",
      " [0.98958415 0.01011005]\n",
      " [0.96879089 0.03107582]\n",
      " ...\n",
      " [0.99732155 0.00277598]\n",
      " [0.99033469 0.00965408]\n",
      " [0.84598517 0.15168542]]\n",
      "Calculated predicitions in 535.0883 seconds,\n",
      "Saved data in 14695.3417 seconds.\n",
      "Evaluated on 2862647 out of 9599237 events\n",
      "Estimated time until completion: 10:31:29.822124\n",
      "Loading file 7/20\n",
      "265690\n",
      "265690\n",
      "Opened data in 908.2384 seconds.\n",
      "[[9.99791086e-01 1.92297899e-04]\n",
      " [9.99964237e-01 3.89939814e-05]\n",
      " [9.99987006e-01 1.41812843e-05]\n",
      " ...\n",
      " [5.55846930e-01 4.43936408e-01]\n",
      " [8.07814837e-01 1.92131266e-01]\n",
      " [9.99999285e-01 8.01143642e-07]]\n",
      "Calculated predicitions in 512.6657 seconds,\n",
      "Saved data in 16101.3602 seconds.\n",
      "Evaluated on 3128337 out of 9599237 events\n",
      "Estimated time until completion: 10:04:04.450247\n",
      "Loading file 8/20\n",
      "194537\n",
      "194537\n",
      "Opened data in 627.9001 seconds.\n",
      "[[9.99685287e-01 3.07287206e-04]\n",
      " [9.95930254e-01 4.19564405e-03]\n",
      " [9.99199569e-01 8.16796499e-04]\n",
      " ...\n",
      " [9.97400880e-01 2.71317852e-03]\n",
      " [9.98718262e-01 1.22366613e-03]\n",
      " [8.81861687e-01 1.16666228e-01]]\n",
      "Calculated predicitions in 469.5521 seconds,\n",
      "Saved data in 17522.6935 seconds.\n",
      "Evaluated on 3322874 out of 9599237 events\n",
      "Estimated time until completion: 9:46:10.404785\n",
      "Loading file 9/20\n",
      "724046\n",
      "724046\n",
      "Opened data in 2627.2088 seconds.\n",
      "[[9.99875188e-01 1.24872735e-04]\n",
      " [9.33237314e-01 6.60943687e-02]\n",
      " [9.99884725e-01 1.12758877e-04]\n",
      " ...\n",
      " [9.97409165e-01 2.58243410e-03]\n",
      " [9.96671021e-01 3.36084818e-03]\n",
      " [9.99496937e-01 4.91209619e-04]]\n",
      "Calculated predicitions in 1734.4619 seconds,\n",
      "Saved data in 18621.6072 seconds.\n",
      "Evaluated on 4046920 out of 9599237 events\n",
      "Estimated time until completion: 8:45:32.753889\n",
      "Loading file 10/20\n",
      "911124\n",
      "911124\n",
      "Opened data in 2592.1379 seconds.\n",
      "[[9.91682649e-01 8.57865345e-03]\n",
      " [9.98998821e-01 9.82282101e-04]\n",
      " [9.96189773e-01 3.92527040e-03]\n",
      " ...\n",
      " [9.96740639e-01 3.37840524e-03]\n",
      " [9.98883784e-01 1.18331634e-03]\n",
      " [9.94396567e-01 5.89927007e-03]]\n",
      "Calculated predicitions in 2084.3711 seconds,\n",
      "Saved data in 22985.1282 seconds.\n",
      "Evaluated on 4958044 out of 9599237 events\n",
      "Estimated time until completion: 7:11:33.889523\n",
      "Loading file 11/20\n",
      "593592\n",
      "593592\n",
      "Opened data in 1386.5864 seconds.\n",
      "[[9.99648333e-01 3.82616214e-04]\n",
      " [9.97281432e-01 2.72556720e-03]\n",
      " [9.97524917e-01 2.31244252e-03]\n",
      " ...\n",
      " [9.93124306e-01 6.65700808e-03]\n",
      " [9.99594152e-01 4.49343730e-04]\n",
      " [9.95576382e-01 4.29845927e-03]]\n",
      "Calculated predicitions in 1067.8562 seconds,\n",
      "Saved data in 27662.6509 seconds.\n",
      "Evaluated on 5551636 out of 9599237 events\n",
      "Estimated time until completion: 6:05:57.853100\n",
      "Loading file 12/20\n",
      "737376\n",
      "737376\n",
      "Opened data in 2053.7295 seconds.\n",
      "[[0.39784697 0.60174274]\n",
      " [0.98538315 0.0152432 ]\n",
      " [0.99824715 0.00173894]\n",
      " ...\n",
      " [0.96780694 0.03192313]\n",
      " [0.94572699 0.05385986]\n",
      " [0.99878758 0.00121712]]\n",
      "Calculated predicitions in 1371.5130 seconds,\n",
      "Saved data in 30118.3579 seconds.\n",
      "Evaluated on 6289012 out of 9599237 events\n",
      "Estimated time until completion: 4:54:15.698105\n",
      "Loading file 13/20\n",
      "215713\n",
      "215713\n",
      "Opened data in 725.6247 seconds.\n",
      "[[0.85954577 0.13979495]\n",
      " [0.50571257 0.49515563]\n",
      " [0.99694961 0.00288703]\n",
      " ...\n",
      " [0.97941226 0.02047827]\n",
      " [0.99813509 0.00170899]\n",
      " [0.9926126  0.00744809]]\n",
      "Calculated predicitions in 420.8785 seconds,\n",
      "Saved data in 33543.9772 seconds.\n",
      "Evaluated on 6504725 out of 9599237 events\n",
      "Estimated time until completion: 4:35:03.406723\n",
      "Loading file 14/20\n",
      "286395\n",
      "286395\n",
      "Opened data in 928.5966 seconds.\n",
      "[[9.99763906e-01 2.44306866e-04]\n",
      " [9.83695030e-01 1.65092591e-02]\n",
      " [9.99123275e-01 8.28729535e-04]\n",
      " ...\n",
      " [8.97882581e-01 1.02054030e-01]\n",
      " [9.71334934e-01 2.82693598e-02]\n",
      " [9.90782201e-01 9.31548979e-03]]\n",
      "Calculated predicitions in 583.9473 seconds,\n",
      "Saved data in 34690.9731 seconds.\n",
      "Evaluated on 6791120 out of 9599237 events\n",
      "Estimated time until completion: 4:09:30.097220\n",
      "Loading file 15/20\n",
      "421478\n",
      "421478\n",
      "Opened data in 1416.2719 seconds.\n",
      "[[9.99973178e-01 2.87779421e-05]\n",
      " [9.99487758e-01 4.82646574e-04]\n",
      " [9.77979362e-01 2.19650939e-02]\n",
      " ...\n",
      " [9.83407438e-01 1.67158209e-02]\n",
      " [9.86497581e-01 1.31861893e-02]\n",
      " [9.56029177e-01 4.31474447e-02]]\n",
      "Calculated predicitions in 821.6004 seconds,\n",
      "Saved data in 36204.2393 seconds.\n",
      "Evaluated on 7212598 out of 9599237 events\n",
      "Estimated time until completion: 3:32:00.445713\n",
      "Loading file 16/20\n",
      "161253\n",
      "161253\n",
      "Opened data in 538.4633 seconds.\n",
      "[[9.78665888e-01 2.16330886e-02]\n",
      " [8.39182436e-01 1.59033582e-01]\n",
      " [9.99731362e-01 2.65507580e-04]\n",
      " ...\n",
      " [9.54213500e-01 4.55881506e-02]\n",
      " [9.95591521e-01 4.46320511e-03]\n",
      " [8.36557269e-01 1.59940124e-01]]\n",
      "Calculated predicitions in 308.0736 seconds,\n",
      "Saved data in 38442.3996 seconds.\n",
      "Evaluated on 7373851 out of 9599237 events\n",
      "Estimated time until completion: 3:17:37.177296\n",
      "Loading file 17/20\n",
      "535009\n",
      "535009\n",
      "Opened data in 1771.8534 seconds.\n",
      "[[9.93304431e-01 6.70373300e-03]\n",
      " [9.99803126e-01 2.09564721e-04]\n",
      " [9.99974608e-01 2.64525752e-05]\n",
      " ...\n",
      " [9.98994291e-01 1.02710316e-03]\n",
      " [9.90704596e-01 8.76522344e-03]\n",
      " [9.99999642e-01 3.91307509e-07]]\n",
      "Calculated predicitions in 1030.4503 seconds,\n",
      "Saved data in 39290.1219 seconds.\n",
      "Evaluated on 7908860 out of 9599237 events\n",
      "Estimated time until completion: 2:29:56.502976\n",
      "Loading file 18/20\n",
      "722035\n",
      "722035\n",
      "Opened data in 2254.8915 seconds.\n",
      "[[9.99895453e-01 1.10920446e-04]\n",
      " [9.99919295e-01 8.62776287e-05]\n",
      " [9.91796970e-01 7.72364857e-03]\n",
      " ...\n",
      " [9.95151281e-01 4.68338886e-03]\n",
      " [9.89192784e-01 1.03784408e-02]\n",
      " [7.37948895e-01 2.61368513e-01]]\n",
      "Calculated predicitions in 1365.1166 seconds,\n",
      "Saved data in 42093.6670 seconds.\n",
      "Evaluated on 8630895 out of 9599237 events\n",
      "Estimated time until completion: 1:25:28.840257\n",
      "Loading file 19/20\n",
      "744719\n",
      "744719\n",
      "Opened data in 2460.8856 seconds.\n",
      "[[9.99995708e-01 4.17299134e-06]\n",
      " [8.83743584e-01 1.17595784e-01]\n",
      " [1.74601469e-02 9.82531786e-01]\n",
      " ...\n",
      " [9.99918818e-01 8.36402760e-05]\n",
      " [8.60495090e-01 1.39505759e-01]\n",
      " [9.82702494e-01 1.70967244e-02]]\n",
      "Calculated predicitions in 1434.7458 seconds,\n",
      "Saved data in 45714.9239 seconds.\n",
      "Evaluated on 9375614 out of 9599237 events\n",
      "Estimated time until completion: 0:19:43.289302\n",
      "Loading file 20/20\n",
      "223623\n",
      "223623\n",
      "Opened data in 740.2572 seconds.\n",
      "[[9.99778569e-01 2.17833134e-04]\n",
      " [5.82806766e-01 4.17711198e-01]\n",
      " [9.99132454e-01 8.66683898e-04]\n",
      " ...\n",
      " [9.98550236e-01 1.34160579e-03]\n",
      " [9.99045312e-01 9.19594022e-04]\n",
      " [1.66337565e-01 8.33027422e-01]]\n",
      "Calculated predicitions in 427.7804 seconds,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved data in 49610.9466 seconds.\n",
      "Evaluated on 9599237 out of 9599237 events\n",
      "Estimated time until completion: 0:00:00\n",
      "Total evaluation time: 50778.9891 seconds.\n"
     ]
    }
   ],
   "source": [
    "nentries = 0\n",
    "print(model_name)\n",
    "t_start = time.time()\n",
    "for i_file, infile_name in enumerate(infiles_list):\n",
    "    \n",
    "    print(\"Loading file {}/{}\".format(i_file+1, len(infiles_list)))\n",
    "    t_filestart = time.time()\n",
    "    \n",
    "    #Open file \n",
    "    file = uproot.open(infile_name)\n",
    "    tree = file[intreename]    \n",
    "    \n",
    "    #Get weights info\n",
    "    dsids = tree[\"DSID\"].array()\n",
    "    mcweights = tree[\"mcWeight\"].array()\n",
    "    NBHadrons = pad_ak(tree[\"Akt10TruthJet_inputJetGABHadrons\"].array(), 30)[:,0]\n",
    "    #### \n",
    "    #For Full decluster\n",
    "    \n",
    "    parent1 = pad_ak3(tree[\"Akt10UFOJet_jetLundIDParent1\"].array(), 2)\n",
    "    parent2 = pad_ak3(tree[\"Akt10UFOJet_jetLundIDParent2\"].array(), 2)\n",
    "    print(len(parent1))\n",
    "    ####\n",
    "    #Get jet kinematics\n",
    "    jet_pts = pad_ak(tree[\"AntiKt10UFOCSSKSoftDropBeta100Zcut10JetsCalib_jetPt\"].array(), 30)[:,0]\n",
    "    jet_etas = pad_ak(tree[\"AntiKt10UFOCSSKSoftDropBeta100Zcut10JetsCalib_jetEta\"].array(), 30)[:,0]\n",
    "    jet_phis = pad_ak(tree[\"AntiKt10UFOCSSKSoftDropBeta100Zcut10JetsCalib_jetPhi\"].array(), 30)[:,0]                      \n",
    "    jet_ms = pad_ak(tree[\"AntiKt10UFOCSSKSoftDropBeta100Zcut10JetsCalib_jetM\"].array(), 30)[:,0]\n",
    "    \n",
    "    #Get lund values\n",
    "    all_lund_zs = pad_ak3(tree[\"{}_jetLundZ\".format(jet_type)].array(), 2)\n",
    "    all_lund_kts = pad_ak3(tree[\"{}_jetLundKt\".format(jet_type)].array(), 2)\n",
    "    all_lund_drs = pad_ak3(tree[\"{}_jetLundDeltaR\".format(jet_type)].array(), 2)\n",
    "    print(len(all_lund_zs))\n",
    "    delta_t_fileax = time.time() - t_filestart\n",
    "    print(\"Opened data in {:.4f} seconds.\".format(delta_t_fileax))\n",
    "     \n",
    "    #### \n",
    "    #For Full decluster\n",
    "    \n",
    "    test_data = create_test_dataset_fulld(all_lund_zs, all_lund_kts, all_lund_drs, parent1, parent2)\n",
    "    \n",
    "    ####\n",
    "    \n",
    "    #test_data = create_test_dataset_prmy(all_lund_zs, all_lund_kts, all_lund_drs)\n",
    "    test_loader = DataLoader(test_data, batch_size=1024, shuffle=False)\n",
    "    \n",
    "    #Predict scores\n",
    "    y_pred = get_scores(test_loader)\n",
    "    print(y_pred)\n",
    "    tagger_scores = y_pred[:,1]\n",
    "    inv_tagger_scores = y_pred[:,0]\n",
    "    delta_t_pred = time.time() - t_filestart - delta_t_fileax\n",
    "    print(\"Calculated predicitions in {:.4f} seconds,\".format(delta_t_pred))\n",
    "\n",
    "    #Save root files containing model scores\n",
    "    filename = infile_name.split(\"/\")[-1]\n",
    "    outfile_path = os.path.join(outdir, filename) \n",
    "    \n",
    "    \n",
    "    with uproot3.recreate(\"{}_score_{}.root\".format(outfile_path, model_name)) as f:\n",
    "\n",
    "        treename = \"FlatSubstructureJetTree\"\n",
    "\n",
    "        #Declare branch data types\n",
    "        f[treename] = uproot3.newtree({\"EventInfo_mcChannelNumber\": \"int32\",\n",
    "                                      \"EventInfo_mcEventWeight\": \"float32\",\n",
    "                                      \"EventInfo_NBHadrons\": \"int32\",   # I doubt saving the parents is necessary here\n",
    "                                      \"fjet_nnscore\": \"float32\",        # which is why I didn't include them\n",
    "                                      \"fjet_invnnscore\": \"float32\", \n",
    "                                      \"fjet_pt\": \"float32\",\n",
    "                                      \"fjet_eta\": \"float32\",\n",
    "                                      \"fjet_phi\": \"float32\",\n",
    "                                      \"fjet_m\": \"float32\",\n",
    "                                      })\n",
    "\n",
    "        #Save branches\n",
    "        f[treename].extend({\"EventInfo_mcChannelNumber\": dsids,\n",
    "                            \"EventInfo_mcEventWeight\": mcweights,\n",
    "                            \"EventInfo_NBHadrons\": NBHadrons,\n",
    "                            \"fjet_nnscore\": tagger_scores, \n",
    "                            \"fjet_invnnscore\": inv_tagger_scores, \n",
    "                            \"fjet_pt\": jet_pts,\n",
    "                            \"fjet_eta\": jet_etas,\n",
    "                            \"fjet_phi\": jet_phis,\n",
    "                            \"fjet_m\": jet_ms,\n",
    "                            })\n",
    "\n",
    "    delta_t_save = time.time() - t_start - delta_t_fileax - delta_t_pred\n",
    "    print(\"Saved data in {:.4f} seconds.\".format(delta_t_save))\n",
    "    \n",
    "    \n",
    "    #Time statistics\n",
    "    nentries += uproot3.numentries(infile_name, intreename)\n",
    "    time_per_entry = (time.time() - t_start)/nentries\n",
    "    eta = time_per_entry * (nentries_total - nentries)\n",
    "    \n",
    "    print(\"Evaluated on {} out of {} events\".format(nentries, nentries_total))    \n",
    "    print(\"Estimated time until completion: {}\".format(str(timedelta(seconds=eta))))\n",
    "    \n",
    "    \n",
    "print(\"Total evaluation time: {:.4f} seconds.\".format(time.time()-t_start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "matched-sweden",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
