{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "compliant-register",
   "metadata": {},
   "source": [
    "https://github.com/sagelywizard/pytorch-mdn/blob/master/mdn/mdn.py\n",
    "\n",
    "That is for the GMM implementation in PyTorch. Very good resource!!\n",
    "\n",
    "https://github.com/ldeecke/gmm-torch/blob/master/gmm.py\n",
    "\n",
    "That is the implementation of the EM algorithm getting the components of a GMM!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "driven-thing",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os.path as osp\n",
    "import os\n",
    "import glob\n",
    "import torch\n",
    "import awkward as ak\n",
    "from torch.utils.data import Dataset\n",
    "import time\n",
    "import uproot\n",
    "import uproot3\n",
    "import numpy as np\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "from torch_geometric.datasets import MNISTSuperpixels\n",
    "from torch_geometric.data import DataListLoader, DataLoader\n",
    "import torch_geometric.transforms as T\n",
    "from torch_geometric.nn import SplineConv, global_mean_pool, DataParallel, EdgeConv\n",
    "from torch_geometric.data import Data\n",
    "from torchsummary import summary\n",
    "from sklearn.neighbors import kneighbors_graph\n",
    "import scipy.sparse as ss\n",
    "from datetime import datetime, timedelta\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "from torch.distributions import Categorical\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GradientReversalFunction(Function):\n",
    "    \"\"\"\n",
    "    Gradient Reversal Layer from:\n",
    "    Unsupervised Domain Adaptation by Backpropagation (Ganin & Lempitsky, 2015)\n",
    "    Forward pass is the identity function. In the backward pass,\n",
    "    the upstream gradients are multiplied by -lambda (i.e. gradient is reversed)\n",
    "    \"\"\"\n",
    "\n",
    "    @staticmethod\n",
    "    def forward(ctx, x, lambda_):\n",
    "        ctx.lambda_ = lambda_\n",
    "        return x.clone()\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grads):\n",
    "        lambda_ = ctx.lambda_\n",
    "        lambda_ = grads.new_tensor(lambda_)\n",
    "        dx = -lambda_ * grads\n",
    "        return dx, None\n",
    "\n",
    "\n",
    "class GradientReversal(torch.nn.Module):\n",
    "    def __init__(self, lambda_=1):\n",
    "        super(GradientReversal, self).__init__()\n",
    "        self.lambda_ = lambda_\n",
    "\n",
    "    def forward(self, x):\n",
    "        return GradientReversalFunction.apply(x, self.lambda_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "different-people",
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_categorical(y, num_classes=None, dtype='float32'):\n",
    "    y = np.array(y, dtype='int')\n",
    "    input_shape = y.shape\n",
    "    if input_shape and input_shape[-1] == 1 and len(input_shape) > 1:\n",
    "        input_shape = tuple(input_shape[:-1])\n",
    "    y = y.ravel()\n",
    "    if not num_classes:\n",
    "        num_classes = np.max(y) + 1\n",
    "    n = y.shape[0]\n",
    "    categorical = np.zeros((n, num_classes), dtype=dtype)\n",
    "    categorical[np.arange(n), y] = 1\n",
    "    output_shape = input_shape + (num_classes,)\n",
    "    categorical = np.reshape(categorical, output_shape)\n",
    "    return categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "empirical-kenya",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_graph_train(z, k, d, p1, p2, label):\n",
    "    vec = []\n",
    "    vec.append(np.array([d, z, k]).T)\n",
    "    vec = np.array(vec)\n",
    "    vec = np.squeeze(vec)\n",
    "\n",
    "    v1 = [[ind, x] for ind, x in enumerate(p1) if x > -1]\n",
    "    v2 = [[ind, x] for ind, x in enumerate(p2) if x > -1]\n",
    "\n",
    "    a1 = np.reshape(v1,(len(v1),2)).T\n",
    "    a2 = np.reshape(v2,(len(v2),2)).T\n",
    "    edge1 = np.concatenate((a1[0], a2[0], a1[1], a2[1]),axis = 0)\n",
    "    edge2 = np.concatenate((a1[1], a2[1], a1[0], a2[0]),axis = 0)\n",
    "    edge = torch.tensor(np.array([edge1, edge2]), dtype=torch.long)\n",
    "    return Data(x=torch.tensor(vec, dtype=torch.float), edge_index=edge, y=torch.tensor(label, dtype=torch.float))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "moderate-denmark",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_train_dataset_fulld(z, k, d, p1, p2, label):\n",
    "    graphs = [create_graph_train(a, b, c, d, e, f) for a, b, c, d, e, f in zip(z, k, d, p1, p2, label)]\n",
    "    #graphs.append(Data(x=torch.tensor(vec, dtype=torch.float), edge_index=edge, y=torch.tensor(label[i], dtype=torch.float)))\n",
    "    return graphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "whole-matrix",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_train_dataset_fulld(z, k, d, p1, p2, label):\n",
    "    graphs = []\n",
    "    for i in range(len(z)):\n",
    "        if i%1000 == 0: \n",
    "            print(\"Processing event {}/{}\".format(i, len(z)), end=\"\\r\")\n",
    "        vec = []\n",
    "        vec.append(np.array([d[i], z[i], k[i]]).T)\n",
    "        vec = np.array(vec)\n",
    "        vec = np.squeeze(vec)\n",
    "\n",
    "        v1 = [[ind, x] for ind, x in enumerate(p1[i]) if x > -1]\n",
    "        v2 = [[ind, x] for ind, x in enumerate(p2[i]) if x > -1]\n",
    "\n",
    "        a1 = np.reshape(v1,(len(v1),2)).T\n",
    "        a2 = np.reshape(v2,(len(v2),2)).T\n",
    "        edge1 = np.concatenate((a1[0], a2[0], a1[1], a2[1]),axis = 0)\n",
    "        edge2 = np.concatenate((a1[1], a2[1], a1[0], a2[0]),axis = 0)\n",
    "        edge = torch.tensor(np.array([edge1, edge2]), dtype=torch.long)\n",
    "        graphs.append(Data(x=torch.tensor(vec, dtype=torch.float), edge_index=edge, y=torch.tensor(label[i], dtype=torch.float)))\n",
    "    return graphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "spectacular-edmonton",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_test_dataset_fulld(z, k, d, p1, p2):\n",
    "    graphs = []\n",
    "    for i in range(len(z)):\n",
    "        vec = []\n",
    "        vec.append(np.array([d[i], z[i], k[i]]).T)\n",
    "        vec = np.array(vec)\n",
    "        vec = np.squeeze(vec)\n",
    "        v1 = [[ind, x] for ind, x in enumerate(p1[i]) if x > -1]\n",
    "        v2 = [[ind, x] for ind, x in enumerate(p2[i]) if x > -1]\n",
    "\n",
    "        a1 = np.reshape(v1,(len(v1),2)).T\n",
    "        a2 = np.reshape(v2,(len(v2),2)).T\n",
    "        edge1 = np.concatenate((a1[0], a2[0], a1[1], a2[1]),axis = 0)\n",
    "        edge2 = np.concatenate((a1[1], a2[1], a1[0], a2[0]),axis = 0)\n",
    "        edge = torch.tensor(np.array([edge1, edge2]), dtype=torch.long)\n",
    "        graphs.append(Data(x=torch.tensor(vec, dtype=torch.float), edge_index=edge))\n",
    "    return graphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "expired-departure",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_train_dataset_prmy(z, k, d, label):\n",
    "    graphs = []\n",
    "    for i in range(len(z)):\n",
    "        vec = []\n",
    "        vec.append(np.array([d[i], z[i], k[i]]).T)\n",
    "        vec = np.array(vec)\n",
    "        vec = np.squeeze(vec)\n",
    "        ya = kneighbors_graph(vec, n_neighbors=int(np.floor(vec.shape[0]/2)))\n",
    "        edges = np.array([ya.nonzero()[0], ya.nonzero()[1]])\n",
    "        edge = torch.tensor(edges, dtype=torch.long)\n",
    "        graphs.append(Data(x=torch.tensor(vec, dtype=torch.float), edge_index=edge, y=torch.tensor(label[i], dtype=torch.float)))\n",
    "    return graphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "wicked-appliance",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_test_dataset_prmy(z, k, d):\n",
    "    graphs = []\n",
    "    for i in range(len(z)):\n",
    "        vec = []\n",
    "        vec.append(np.array([d[i], z[i], k[i]]).T)\n",
    "        vec = np.array(vec)\n",
    "        vec = np.squeeze(vec)\n",
    "        ya = kneighbors_graph(vec, n_neighbors=int(np.floor(vec.shape[0]/2)))\n",
    "        edges = np.array([ya.nonzero()[0], ya.nonzero()[1]])\n",
    "        edge = torch.tensor(edges, dtype=torch.long)\n",
    "        graphs.append(Data(x=torch.tensor(vec, dtype=torch.float), edge_index=edge))\n",
    "    return graphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "coastal-muslim",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Configuration from bash script\n",
    "if \"INFILE\" in os.environ:\n",
    "    infile_path = os.environ[\"INFILE\"]\n",
    "    model_name = os.environ[\"MODEL\"]\n",
    "    epochs = int(os.environ[\"EPOCHS\"])\n",
    "\n",
    "#Configuration in notebook\n",
    "else:    \n",
    "#    files = glob.glob(\"/mnt/storage/asopio/train_test_split_20210305/training_set_jz2to9.root\")\n",
    "    files = glob.glob(\"/mnt/storage/raresiora/train_test_split/full_decluster/wprime/*_train.root\") + glob.glob(\"/mnt/storage/raresiora/train_test_split/full_decluster/jz3to9/*_train.root\")\n",
    "    model_name = \"GNN_full\"\n",
    "#    model_name = \"LSTM\"\n",
    "#    model_name = \"1DCNN\"\n",
    "#    model_name = \"2DCNN\"\n",
    "#    model_name = \"ImgCNN\"\n",
    "#    model_name = \"GNN\"\n",
    "#    model_name = \"Transformer\"\n",
    "#    nb_epochs = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "furnished-dealing",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/mnt/storage/raresiora/train_test_split/full_decluster/wprime/user.asopio.24603642._000001.ANALYSIS.root_train.root\n",
      "Evaluating on 16 files with 334337 entries in total.\n"
     ]
    }
   ],
   "source": [
    "nentries_total = 0\n",
    "intreename = \"lundjets_InDetTrackParticles\"\n",
    "print(files[0])\n",
    "for infile_name in files: \n",
    "    nentries_total += uproot3.numentries(infile_name, intreename)\n",
    "\n",
    "print(\"Evaluating on {} files with {} entries in total.\".format(len(file3), nentries_total))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "weird-award",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load tf keras model\n",
    "# jet_type = \"Akt10RecoChargedJet\" #track jets\n",
    "jet_type = \"Akt10UFOJet\" #UFO jets\n",
    "\n",
    "save_trained_model = True\n",
    "intreename = \"lundjets_InDetTrackParticles\"\n",
    "\n",
    "model_filename = \"save/models/\"+model_name+\".hdf5\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "official-challenge",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training tagger on files 22\n",
      "Loading file /mnt/storage/raresiora/train_test_split/full_decluster/wprime/user.asopio.24603642._000001.ANALYSIS.root_train.root\n",
      "Loading file /mnt/storage/raresiora/train_test_split/full_decluster/wprime/user.asopio.24603642._000004.ANALYSIS.root_train.root\n",
      "Loading file /mnt/storage/raresiora/train_test_split/full_decluster/wprime/user.asopio.24603642._000008.ANALYSIS.root_train.root\n",
      "Loading file /mnt/storage/raresiora/train_test_split/full_decluster/wprime/user.asopio.24603642._000003.ANALYSIS.root_train.root\n",
      "Loading file /mnt/storage/raresiora/train_test_split/full_decluster/wprime/user.asopio.24603642._000012.ANALYSIS.root_train.root\n",
      "Loading file /mnt/storage/raresiora/train_test_split/full_decluster/j3to9/user.asopio.24603633._000004.ANALYSIS.root_train.root\n",
      "Loading file /mnt/storage/raresiora/train_test_split/full_decluster/j3to9/user.asopio.24603631._000012.ANALYSIS.root_train.root\n",
      "Loading file /mnt/storage/raresiora/train_test_split/full_decluster/j3to9/user.asopio.24603627._000002.ANALYSIS.root_train.root\n",
      "Loading file /mnt/storage/raresiora/train_test_split/full_decluster/j3to9/user.asopio.24603627._000008.ANALYSIS.root_train.root\n",
      "Loading file /mnt/storage/raresiora/train_test_split/full_decluster/j3to9/user.asopio.24603626._000018.ANALYSIS.root_train.root\n",
      "Loading file /mnt/storage/raresiora/train_test_split/full_decluster/j3to9/user.asopio.24603630._000002.ANALYSIS.root_train.root\n",
      "Loading file /mnt/storage/raresiora/train_test_split/full_decluster/j3to9/user.asopio.24603628._000032.ANALYSIS.root_train.root\n",
      "Loading file /mnt/storage/raresiora/train_test_split/full_decluster/j3to9/user.asopio.24603626._000004.ANALYSIS.root_train.root\n",
      "Loading file /mnt/storage/raresiora/train_test_split/full_decluster/j3to9/user.asopio.24603632._000003.ANALYSIS.root_train.root\n",
      "Loading file /mnt/storage/raresiora/train_test_split/full_decluster/j3to9/user.asopio.24603628._000031.ANALYSIS.root_train.root\n",
      "Loading file /mnt/storage/raresiora/train_test_split/full_decluster/j3to9/user.asopio.24603630._000009.ANALYSIS.root_train.root\n",
      "Loading file /mnt/storage/raresiora/train_test_split/full_decluster/j3to9/user.asopio.24603632._000007.ANALYSIS.root_train.root\n",
      "Loading file /mnt/storage/raresiora/train_test_split/full_decluster/j3to9/user.asopio.24603628._000046.ANALYSIS.root_train.root\n",
      "Loading file /mnt/storage/raresiora/train_test_split/full_decluster/j3to9/user.asopio.24603632._000002.ANALYSIS.root_train.root\n",
      "Loading file /mnt/storage/raresiora/train_test_split/full_decluster/j3to9/user.asopio.24603631._000003.ANALYSIS.root_train.root\n",
      "Loading file /mnt/storage/raresiora/train_test_split/full_decluster/j3to9/user.asopio.24603627._000011.ANALYSIS.root_train.root\n",
      "Loading file /mnt/storage/raresiora/train_test_split/full_decluster/j3to9/user.asopio.24603631._000007.ANALYSIS.root_train.root\n",
      "Opened data in 3531.7292 seconds.\n"
     ]
    }
   ],
   "source": [
    "def pad_ak(arr, l):\n",
    "    arr = ak.pad_none(arr, l)\n",
    "    arr = ak.fill_none(arr, 1)\n",
    "    arr = ak.to_numpy(arr)\n",
    "    return arr\n",
    "\n",
    "def pad_ak3(arr, l):\n",
    "    arr = ak.pad_none(arr, 1)\n",
    "    arr = ak.fill_none(arr, [0])\n",
    "    arr = arr[:,0]\n",
    "    arr = ak.pad_none(arr, l)\n",
    "    arr = ak.fill_none(arr, 0)\n",
    "    #arr = ak.to_numpy(arr)\n",
    "    return arr\n",
    "\n",
    "print(\"Training tagger on files\", len(files))\n",
    "t_start = time.time()\n",
    "\n",
    "dsids = np.array([])\n",
    "NBHadrons = np.array([])\n",
    "trial = np.ones((1,30))\n",
    "all_lund_zs = ak.from_numpy(trial)\n",
    "all_lund_kts = ak.from_numpy(trial)\n",
    "all_lund_drs = ak.from_numpy(trial)\n",
    "parent1 = ak.from_numpy(trial)\n",
    "parent2 = ak.from_numpy(trial)\n",
    "jet_pts = np.array([])\n",
    "jet_ms = np.array([])\n",
    "eta = np.array([])\n",
    "vector = []\n",
    "for file in files:\n",
    "    \n",
    "    print(\"Loading file\",file)\n",
    "    \n",
    "    infile = uproot.open(file)\n",
    "    tree = infile[intreename]\n",
    "    dsids = np.append( dsids, np.array(tree[\"DSID\"].array()) )\n",
    "    #print(tree.keys())\n",
    "    #eta = ak.concatenate(eta, pad_ak3(tree[\"Akt10TruthJet_jetEta\"].array(), 30),axis=0)\n",
    "    mcweights = tree[\"mcWeight\"].array()\n",
    "    NBHadrons = np.append( NBHadrons, pad_ak(tree[\"Akt10TruthJet_inputJetGABHadrons\"].array(), 30)[:,0] )\n",
    "    parent1 = ak.concatenate((parent1, pad_ak3(tree[\"{}_jetLundIDParent1\".format(jet_type)].array(), 2)), axis = 0)\n",
    "    parent2 = ak.concatenate((parent2, pad_ak3(tree[\"{}_jetLundIDParent2\".format(jet_type)].array(), 2)), axis = 0)\n",
    "    \n",
    "    #Get jet kinematics\n",
    "    jet_pts = np.append(jet_pts, pad_ak(tree[\"{}_jetPt\".format(jet_type)].array(), 30)[:,0])\n",
    "    #jet_etas = pad_ak(tree[\"AntiKt10UFOCSSKSoftDropBeta100Zcut10JetsCalib_jetEta\"].array(), 30)[:,0]\n",
    "    #jet_phis = pad_ak(tree[\"AntiKt10UFOCSSKSoftDropBeta100Zcut10JetsCalib_jetPhi\"].array(), 30)[:,0]              \n",
    "    jet_ms = np.append(jet_ms, pad_ak(tree[\"{}_jetM\".format(jet_type)].array(), 30)[:,0])\n",
    "    \n",
    "    #Get Lund variables\n",
    "    all_lund_zs = ak.concatenate((all_lund_zs, pad_ak3(tree[\"{}_jetLundZ\".format(jet_type)].array(), 2)), axis=0)\n",
    "    all_lund_kts = ak.concatenate((all_lund_kts, pad_ak3(tree[\"{}_jetLundKt\".format(jet_type)].array(), 2)), axis=0)\n",
    "    all_lund_drs = ak.concatenate((all_lund_drs, pad_ak3(tree[\"{}_jetLundDeltaR\".format(jet_type)].array(), 2)), axis=0)\n",
    "    #print(len(jet_pts), len(jet_ms))\n",
    "all_lund_zs = all_lund_zs[1:]    \n",
    "all_lund_kts = all_lund_kts[1:]\n",
    "all_lund_drs = all_lund_drs[1:]\n",
    "parent1 = parent1[1:]\n",
    "parent2 = parent2[1:]\n",
    "\n",
    "\n",
    "delta_t_fileax = time.time() - t_start\n",
    "print(\"Opened data in {:.4f} seconds.\".format(delta_t_fileax))\n",
    "\n",
    "#Get labels\n",
    "#labels = ( dsids > 360000 ) & ( dsids < 370000 )\n",
    "\n",
    "labels = ( dsids > 370000 ) # depends on your signal and background definition\n",
    "\n",
    "#print(labels)\n",
    "labels = to_categorical(labels, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "green-navigation",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Just random numbers to check the models work\n",
    "# These are not used in practice\n",
    "all_lund_zs = np.random.randn(50000,30)\n",
    "all_lund_kts = np.random.randn(50000,30)\n",
    "all_lund_drs = np.random.randn(50000,30)\n",
    "parent1 = np.random.randint(low=0, high=20, size=(50000, 14))\n",
    "parent2 = np.random.randint(low=0, high=20, size=(50000, 14))\n",
    "labels = np.random.randint(low=0, high=1, size=(50000, 2))\n",
    "jet_pts = np.random.randint(low=1, high=500, size=(50000))\n",
    "jet_ms = np.random.randint(low=1, high=500, size=(50000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "lovely-cutting",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing event 1242000/1242295\r"
     ]
    }
   ],
   "source": [
    "#W bosons\n",
    "dataset = create_train_dataset_fulld(all_lund_zs, all_lund_kts, all_lund_drs, parent1, parent2, labels) \n",
    "train_loader = DataLoader(dataset, batch_size=1024, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "palestinian-donna",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.conv1 = EdgeConv(nn.Sequential(nn.Linear(6, 64),\n",
    "                                  nn.ReLU(), nn.Linear(64, 64), nn.ReLU(), nn.Linear(64, 64)),aggr='add')\n",
    "        self.conv2 = EdgeConv(nn.Sequential(nn.Linear(128, 128),\n",
    "                                  nn.ReLU(), nn.Linear(128, 128),nn.ReLU(), nn.Linear(128, 128)),aggr='add')\n",
    "        self.conv3 = EdgeConv(nn.Sequential(nn.Linear(256,256,),\n",
    "                                  nn.ReLU(), nn.Linear(256, 256),nn.ReLU(), nn.Linear(256, 256)),aggr='add')\n",
    "        self.lin1 = torch.nn.Linear(256, 128)\n",
    "        self.lin2 = torch.nn.Linear(256, 64)\n",
    "        self.lin3 = torch.nn.Linear(64, 2)\n",
    "        \n",
    "    def forward(self, data):\n",
    "        x, edge_index, batch = data.x, data.edge_index, data.batch\n",
    "        x = self.conv1(x, edge_index)\n",
    "        x = self.conv2(x, edge_index)\n",
    "        x = self.conv3(x, edge_index)\n",
    "        x = global_mean_pool(x, batch)\n",
    "        x = F.dropout(x, p=0.1)\n",
    "        x = self.lin2(x)\n",
    "        x = F.dropout(x, p=0.1)\n",
    "        x = self.lin3(x)\n",
    "        #print(x.shape)\n",
    "        return F.sigmoid(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "phantom-communications",
   "metadata": {},
   "source": [
    "Part to train the classifier. Can be skipped where you just import a trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "alleged-lawyer",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Net()\n",
    "device = torch.device('cuda') # Usually gpu 4 worked best, it had the most memory available\n",
    "model.to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.0005)\n",
    "\n",
    "def train(epoch):\n",
    "    model.train()\n",
    "\n",
    "    loss_all = 0\n",
    "    for data in train_loader:\n",
    "        data = data.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        new_y = torch.reshape(data.y, (int(list(data.y.shape)[0]/2),2))\n",
    "        loss = F.binary_cross_entropy(output, new_y)\n",
    "        loss.backward()\n",
    "        loss_all += data.num_graphs * loss.item()\n",
    "        optimizer.step()\n",
    "    return loss_all / len(dataset)\n",
    "\n",
    "@torch.no_grad()\n",
    "def get_accuracy(loader):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    for data in loader:\n",
    "        data = data.to(device)\n",
    "        new_y = torch.reshape(data.y, (int(list(data.y.shape)[0]/2),2))\n",
    "        pred = model(data).max(dim=1)[1]\n",
    "        correct += pred.eq(new_y[:,1]).sum().item()\n",
    "    return correct / len(loader.dataset)\n",
    "    \n",
    "@torch.no_grad()\n",
    "def get_scores(loader):\n",
    "    model.eval()\n",
    "    total_output = np.array([[1,1]])\n",
    "    for data in loader:\n",
    "        data = data.to(device)\n",
    "        pred = model(data)\n",
    "        total_output = np.append(total_output, pred.cpu().detach().numpy(), axis=0)\n",
    "    return total_output[1:]\n",
    "    \n",
    "for epoch in range(1, 21):\n",
    "    loss = train(epoch)\n",
    "    train_acc = get_accuracy(train_loader)\n",
    "    print('Epoch: {:03d}, Loss: {:.5f}, Train Acc: {:.5f}'.format(epoch, loss, train_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "liable-money",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"/your/path/to/models/wprime.pt\"\n",
    "torch.save(model.state_dict(), path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bored-underwear",
   "metadata": {},
   "source": [
    "This is only for loading the trained classifier for mass decorrelation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "complimentary-cycle",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"/your/path/to/models/wprime.pt\"\n",
    "clsf = Net()\n",
    "clsf.load_state_dict(torch.load(path))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "collectible-remedy",
   "metadata": {},
   "source": [
    "Mixture density network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "consolidated-nickname",
   "metadata": {},
   "outputs": [],
   "source": [
    "ONEOVERSQRT2PI = 1.0 / math.sqrt(2 * math.pi)\n",
    "\n",
    "\n",
    "class MDN(nn.Module):\n",
    "    \"\"\"A mixture density network layer\n",
    "    The input maps to the parameters of a MoG probability distribution, where\n",
    "    each Gaussian has O dimensions and diagonal covariance.\n",
    "    Arguments:\n",
    "        in_features (int): the number of dimensions in the input\n",
    "        out_features (int): the number of dimensions in the output\n",
    "        num_gaussians (int): the number of Gaussians per output dimensions\n",
    "    Input:\n",
    "        minibatch (BxD): B is the batch size and D is the number of input\n",
    "            dimensions.\n",
    "    Output:\n",
    "        (pi, sigma, mu) (BxG, BxGxO, BxGxO): B is the batch size, G is the\n",
    "            number of Gaussians, and O is the number of dimensions for each\n",
    "            Gaussian. Pi is a multinomial distribution of the Gaussians. Sigma\n",
    "            is the standard deviation of each Gaussian. Mu is the mean of each\n",
    "            Gaussian.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, in_features, out_features, num_gaussians):\n",
    "        super(MDN, self).__init__()\n",
    "        self.in_features = in_features\n",
    "        self.out_features = out_features\n",
    "        self.num_gaussians = num_gaussians\n",
    "        self.pi = nn.Sequential(\n",
    "            nn.Linear(in_features, num_gaussians),\n",
    "            nn.Softmax(dim=1)\n",
    "        )\n",
    "        self.sigma = nn.Linear(in_features, out_features * num_gaussians)\n",
    "        self.mu = nn.Linear(in_features, out_features * num_gaussians)\n",
    "\n",
    "    def forward(self, minibatch):\n",
    "        pi = self.pi(minibatch)\n",
    "        sigma = torch.exp(self.sigma(minibatch))\n",
    "        sigma = sigma.view(-1, self.num_gaussians, self.out_features)\n",
    "        mu = self.mu(minibatch)\n",
    "        mu = mu.view(-1, self.num_gaussians, self.out_features)\n",
    "        return pi, sigma, mu\n",
    "\n",
    "\n",
    "def gaussian_probability(sigma, mu, target):\n",
    "    \"\"\"Returns the probability of `target` given MoG parameters `sigma` and `mu`.\n",
    "    Arguments:\n",
    "        sigma (BxGxO): The standard deviation of the Gaussians. B is the batch\n",
    "            size, G is the number of Gaussians, and O is the number of\n",
    "            dimensions per Gaussian.\n",
    "        mu (BxGxO): The means of the Gaussians. B is the batch size, G is the\n",
    "            number of Gaussians, and O is the number of dimensions per Gaussian.\n",
    "        target (BxI): A batch of target. B is the batch size and I is the number of\n",
    "            input dimensions.\n",
    "    Returns:\n",
    "        probabilities (BxG): The probability of each point in the probability\n",
    "            of the distribution in the corresponding sigma/mu index.\n",
    "    \"\"\"\n",
    "    target = target.unsqueeze(1).expand_as(sigma)\n",
    "    ret = ONEOVERSQRT2PI * torch.exp(-0.5 * ((target - mu) / sigma)**2) / sigma\n",
    "    ret = torch.where(ret == 0, ret + 1E-20, ret)\n",
    "    return torch.prod(ret, 2)\n",
    "\n",
    "\n",
    "def mdn_loss(pi, sigma, mu, target):\n",
    "    \"\"\"Calculates the error, given the MoG parameters and the target\n",
    "    The loss is the negative log likelihood of the data given the MoG\n",
    "    parameters.\n",
    "    \"\"\"\n",
    "    prob = pi * gaussian_probability(sigma, mu, target)\n",
    "    nll = -torch.log(torch.sum(prob, dim=1))\n",
    "    return torch.mean(nll)\n",
    "\n",
    "\n",
    "def sample(pi, sigma, mu):\n",
    "    \"\"\"Draw samples from a MoG.\n",
    "    \"\"\"\n",
    "    # Choose which gaussian we'll sample from\n",
    "    pis = Categorical(pi).sample().view(pi.size(0), 1, 1)\n",
    "    # Choose a random sample, one randn for batch X output dims\n",
    "    # Do a (output dims)X(batch size) tensor here, so the broadcast works in\n",
    "    # the next step, but we have to transpose back.\n",
    "    gaussian_noise = torch.randn(\n",
    "        (sigma.size(2), sigma.size(0)), requires_grad=False)\n",
    "    variance_samples = sigma.gather(1, pis).detach().squeeze()\n",
    "    mean_samples = mu.detach().gather(1, pis).squeeze()\n",
    "    return (gaussian_noise * variance_samples + mean_samples).transpose(0, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fallen-waste",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_gaussians = 20 # number of Gaussians at the end\n",
    "\n",
    "# The architecture of the adversary could be changed\n",
    "class Adversary(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Adversary, self).__init__()\n",
    "        self.gauss = nn.Sequential(\n",
    "    nn.Linear(2, 64),\n",
    "    nn.ReLU(),\n",
    "    MDN(64, 1, num_gaussians)\n",
    ")\n",
    "        self.revgrad = GradientReversal(10)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.revgrad(x) # important hyperparameter, the scale, \n",
    "                                     # tells by how much the classifier is punished\n",
    "        x = self.gauss(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "rolled-cinema",
   "metadata": {},
   "source": [
    "Setting up the mass and pts for the adversarial network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "amber-positive",
   "metadata": {},
   "outputs": [],
   "source": [
    "ms = np.array(jet_ms).reshape(len(jet_ms), 1)\n",
    "pts = np.array(np.log(jet_pts)).reshape(len(jet_pts), 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "decent-veteran",
   "metadata": {},
   "source": [
    "Creating the adversarial dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "suited-vatican",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_adversary_trainset(pt, mass):\n",
    "    graphs = [Data(x=torch.tensor([p], dtype=torch.float), y=torch.tensor([m], dtype=torch.float)) for p, m in zip(pt, mass)]\n",
    "    return graphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "confident-stability",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConcatDataset(Dataset):\n",
    "    def __init__(self, datasetA, datasetB):\n",
    "        self.datasetA = datasetA\n",
    "        self.datasetB = datasetB\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        xA = self.datasetA[index]\n",
    "        xB = self.datasetB[index]\n",
    "        return xA, xB\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.datasetA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "answering-debut",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda') # Usually gpu 4 worked best, it had the most memory available\n",
    "clsf.to(device)\n",
    "adv.to(device)\n",
    "optimizer = torch.optim.Adam(adv.parameters(), lr=0.0005)\n",
    "#optimizer = torch.optim.Adam(list(clsf.parameters()) + list(adv.parameters()), lr=0.0005)\n",
    "\n",
    "def train(epoch):\n",
    "    clsf.eval()\n",
    "    adv.train()\n",
    "    loss_all = 0\n",
    "    for data in adv_loader:\n",
    "        cl_data = data[0].to(device)\n",
    "        adv_data = data[1].to(device)\n",
    "        new_y = torch.reshape(cl_data.y, (int(list(cl_data.y.shape)[0]),1))\n",
    "        mask_bkg = new_y.lt(0.5)\n",
    "        optimizer.zero_grad()\n",
    "        cl_out = clsf(cl_data)\n",
    "        #print(torch.reshape(cl_out, (len(cl_out), 1)), torch.reshape(cl_out, (len(cl_out), 1)).shape)\n",
    "        #print(adv_data.x, adv_data.x.shape)\n",
    "        adv_inp = torch.cat((torch.reshape(cl_out[mask_bkg], (len(cl_out[mask_bkg]), 1)), torch.reshape(adv_data.x[mask_bkg], (len(adv_data.x[mask_bkg]), 1))), 1)\n",
    "        #print(adv_inp.shape)\n",
    "        pi, sigma, mu = adv(adv_inp)\n",
    "        #cl_out = clsf(cl_data)\n",
    "        loss2 = mdn_loss(pi, sigma, mu, torch.reshape(adv_data.y[mask_bkg], (len(cl_out[mask_bkg]), 1)))\n",
    "        loss2.backward()\n",
    "        loss_all += cl_data.num_graphs * loss2.item()\n",
    "        optimizer.step()\n",
    "    return loss_all / len(dataset)\n",
    "\n",
    "@torch.no_grad()\n",
    "def get_accuracy(loader):\n",
    "    #remember to change this when evaluating combined model\n",
    "    clsf.eval()\n",
    "    correct = 0\n",
    "    for data in loader:\n",
    "        cl_data = data[0].to(device)\n",
    "        #adv_data = data[1].to(device)\n",
    "        new_y = torch.reshape(cl_data.y, (int(list(cl_data.y.shape)[0]),1))\n",
    "        pred = clsf(cl_data).max(dim=1)[1]\n",
    "        correct += pred.eq(new_y[:,1]).sum().item()\n",
    "    return correct / len(loader.dataset)\n",
    "    \n",
    "@torch.no_grad()\n",
    "def get_scores(loader):\n",
    "    clsf.eval()\n",
    "    total_output = np.array([[1,1]])\n",
    "    for data in loader:\n",
    "        cl_data = data[0].to(device)\n",
    "        pred = clsf(cl_data)\n",
    "        total_output = np.append(total_output, pred.cpu().detach().numpy(), axis=0)\n",
    "    return total_output[1:]\n",
    "    \n",
    "\n",
    "print(\"Training adversary whilst keeping classifier the same.\")\n",
    "\n",
    "for epoch in range(1, 51): # this may need to be bigger\n",
    "    loss = train(epoch)\n",
    "   # train_acc = get_accuracy(adv_loader)\n",
    "    print('Epoch: {:03d}, Loss: {:.5f}, Train Acc: - '.format(epoch, loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for param in clsf.parameters():\n",
    "    param.require_grads = True\n",
    "\n",
    "device = torch.device('cuda') # Usually gpu 4 worked best, it had the most memory available\n",
    "clsf.to(device)\n",
    "adv.to(device)\n",
    "optimizer_cl = torch.optim.Adam(clsf.parameters(), lr=0.01)\n",
    "optimizer_adv = torch.optim.Adam(adv.parameters(), lr=0.00001)\n",
    "#optimizer = torch.optim.Adam(list(clsf.parameters()) + list(adv.parameters()), lr=0.0005)\n",
    "\n",
    "\n",
    "def train(epoch):\n",
    "    clsf.train()\n",
    "    adv.train()\n",
    "    loss_all = 0\n",
    "    for data in adv_loader:\n",
    "        cl_data = data[0].to(device)\n",
    "        adv_data = data[1].to(device)\n",
    "        new_y = torch.reshape(cl_data.y, (int(list(cl_data.y.shape)[0]),1))\n",
    "        mask_bkg = new_y.lt(0.5)\n",
    "        optimizer_cl.zero_grad()\n",
    "        optimizer_adv.zero_grad()\n",
    "        cl_out = clsf(cl_data)\n",
    "        adv_inp = torch.cat((torch.reshape(cl_out[mask_bkg], (len(cl_out[mask_bkg]), 1)), torch.reshape(adv_data.x[mask_bkg], (len(adv_data.x[mask_bkg]), 1))), 1)\n",
    "        pi, sigma, mu = adv(adv_inp)\n",
    "        loss1 = F.binary_cross_entropy(cl_out, new_y)\n",
    "        loss2 = mdn_loss(pi, sigma, mu, torch.reshape(adv_data.y[mask_bkg], (len(adv_data.y[mask_bkg]), 1)))\n",
    "        loss = loss1 + loss2\n",
    "        loss.backward()\n",
    "        loss_all += cl_data.num_graphs * loss.item()\n",
    "        optimizer_cl.step()\n",
    "        optimizer_adv.step()\n",
    "    return loss_all / len(dataset)\n",
    "\n",
    "print(\"Started training together!\")\n",
    "\n",
    "for epoch in range(1, 21): # this may need to be bigger\n",
    "    loss = train(epoch)\n",
    "    #train_acc = get_accuracy(adv_loader)\n",
    "    print('Epoch: {:03d}, Loss: {:.5f}, Train Acc:'.format(epoch, loss))\n",
    "\n",
    "path = \"/your/path/to/models/lundnet_2opt_2_5.pt\"\n",
    "torch.save(clsf.state_dict(), path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
